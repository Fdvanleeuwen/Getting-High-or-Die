---
title: "Supervised Learning and Visualisation: Assignment 2"
author: "Group 1: Florian van Leeuwen, AlexCarriero, Christoph Voltzke, Judith Neve"
date: "13/11/2022"
output: html_document
---

---

--- title page --- how do we do this?? there's not multiple pages on html
--- add the tabs --- shannon pls help us

```{r setup, echo=FALSE}
library(knitr)
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE,
  echo = FALSE
)
```

```{r libraries}
library(readr)
library(tidyverse)
library(psych)
library(MASS)
library(pROC)
library(randomForest)
library(xgboost)
library(rpart)
library(rpart.plot)
library(jtools)
library(class)
library(ISLR)
library(Matrix)
library(kableExtra)
```

## Introduction

The focus of this project is the accurate classification of mushrooms as edible or poisonous using an [open dataset from Kaggle](https://www.kaggle.com/datasets/uciml/mushroom-classification). The end goal is to develop an app for mushroom foragers of all levels, that will reliably classify a mushroom as edible or poisonous. In this project, we develop the classification algorithms that will be used within this app. We predict whether a mushroom is edible, that is, the given probabilities are the probability that a mushroom is edible.\
\
The dataset contains 8124 observations of 22 variables (plus the class). All variables are categorical and describe various characteristics of mushrooms, detailed in Table 1. These characteristics range from easily understandable (e.g., cap colour) to senseless for people unfamiliar with mushrooms (e.g., stalk root). In order for our app to be usable by all, we reduce the number of variables in order to only include the most easily understandable variables. Furthermore, we reduce the number of options for some of these variables by grouping certain categories together in order to limit misspecification of predictors (e.g., brown and buff will be both called brown).\
\
We will first develop models using only this reduced set of predictors; if model performance is unsatisfactory, we will consider some further models that include more predictors.

```{r import data}
# import data
mushrooms <- read.csv("mushrooms.csv")
```

```{r tidy data}
# tidy data -- merge redundant categories 
mushrooms <- mushrooms %>% 
    mutate(across(where(is.character), as_factor))%>%
    dplyr::select(-veil.type, -stalk.root)%>%     
    mutate(cap.shape = cap.shape %>% 
             fct_recode("bell"    = "b",
                        "conical" = "c",
                        "convex"  = "x",
                        "flat"    = "f", 
                        "knobbed" = "k",
                        "sunken"  = "s"),
           cap.surface = cap.surface %>% 
             fct_recode("fibrous" = "f",
                        "grooves" = "g",
                        "scaly"   = "y",
                        "smooth"  = "s"), 
           bruises = bruises %>% 
             fct_recode("true"       = "t",
                        "false"      = "f"), 
           odor = odor %>%
             fct_recode("almond"     = "a",
                        "anise"      = "l",
                        "creosote"   = "c",
                        "fishy"      = "y",
                        "foul"       = "f",
                        "musty"      = "m",
                        "none"       = "n",
                        "pungent"    = "p",
                        "spicy"      = "s"),
           cap.color = cap.color %>%                 
             fct_recode("brown"      = "n",
                        "brown"      = "b",
                        "brown"      = "c", 
                        "pink"       = "u",                      
                        "pink"       = "e", 
                        "pink"       = "p", 
                        "gray"       = "g",
                        "green"      = "r", 
                        "white"      = "w",
                        "yellow"     = "y"),
           gill.attachment = gill.attachment %>% 
             fct_recode("attached"   = "a",
                        "free"       = "f"), 
           gill.spacing = gill.spacing %>% 
             fct_recode("close"      = "c",
                        "crowded"    = "w"),
           gill.size = gill.size %>% 
             fct_recode("broad"      = "b",
                        "narrow"     = "n"), 
           gill.color = gill.color %>% 
             fct_recode("black"      ="k",
                        "brown"      ="n",
                        "brown"      ="b",
                        "brown"      ="h",
                        "gray"       ="g", 
                        "green"      ="r",
                        "orange"     ="o",
                        "pink"       ="p",
                        "pink"       ="u",
                        "pink"       ="e",
                        "white"      ="w",
                        "yellow"     ="y"),
           stalk.shape = stalk.shape %>% 
             fct_recode("enlarging"  ="e",
                        "tapering"   ="t"), 
           stalk.surface.above.ring = stalk.surface.above.ring %>% 
             fct_recode("fibrous"    ="f",
                        "scaly"      ="y",
                        "silky"      ="k",
                        "smooth"     ="s"), 
          stalk.surface.below.ring = stalk.surface.below.ring %>% 
             fct_recode("fibrous"    ="f",
                        "scaly"      ="y",
                        "silky"      ="k",
                        "smooth"     ="s"), 
          stalk.color.above.ring = stalk.color.above.ring %>% 
            fct_recode("brown"       ="n",
                       "brown"       ="b",
                       "brown"       ="c",
                       "gray"        ="g",
                       "orange"      ="o",
                       "pink"        ="p",
                       "pink"        ="e",
                       "white"       ="w",
                       "yellow"      ="y",), 
          stalk.color.below.ring = stalk.color.below.ring %>% 
            fct_recode("brown"       ="n",
                       "brown"       ="b",
                       "brown"       ="c",
                       "gray"        ="g",
                       "orange"      ="o",
                       "pink"        ="p",
                       "pink"        ="e",
                       "white"       ="w",
                       "yellow"      ="y",), 
          veil.color = veil.color %>%
            fct_recode("brown"       ="n",
                       "orange"      ="o",
                       "white"       ="w",
                       "yellow"      ="y"),
          ring.number = ring.number %>% 
            fct_recode("none"        ="n",
                       "one"         ="o",
                       "two"         ="t"), 
          ring.type = ring.type %>% 
            fct_recode("evanescent"  ="e",
                       "flaring"     ="f",
                       "large"       ="l",
                       "none"        ="n",
                       "pendant"     ="p"),
          spore.print.color = spore.print.color %>% 
            fct_recode("black"   ="k",
                       "brown"   ="n",
                       "brown"   ="b",
                       "brown"   ="h",
                       "green"   ="r",
                       "orange"  ="o",
                       "purple"  ="u",
                       "white"   ="w",
                       "yellow"  ="y"), 
         population = population %>% 
           fct_recode("group"    ="a",
                      "group"    ="c",
                      "group"    ="n",
                      "group"    ="s",
                      "group"    ="v",
                      "solitary" ="y"), 
         habitat = habitat %>% 
           fct_recode("grasses"  ="g",
                      "woods"    ="l",
                      "grasses"  ="m",
                      "urban"    ="p",
                      "urban"    ="u",
                      "waste"    ="w",
                      "woods"    ="d"),
         class = ifelse(class == "e", 1, 0))
```

```{r}
table1 <- tibble(
  Variable = c(
    "cap shape",
    "cap surface",
    "cap color",
    "bruises",
    "odor",
    "gill attachment",
    "gill spacing",
    "gill size",
    "gill color",
    "stalk shape",
    "stalk root",
    "stalk surface above ring",
    "stalk surface below ring",
    "stalk color above ring",
    "stalk color below ring",
    "veil type",
    "veil color",
    "ring number",
    "ring type",
    "spore print color",
    "population",
    "habitat"
  ),
  `Original levels` = c(
    "bell, conical, convex, flat, knobbed, sunken",
    "fibrous, grooves, scaly, smooth",
    "brown, buff, cinnamon, gray, green, pink, purple, red, white, yellow",
    "yes, no",
    "almond, anise, creosote, fishy, foul, musty, none, pungent, spicy",
    "attached, descending, free, notched",
    "close, crowded, distant",
    "broad, narrow",
    "black, brown, buff, chocolate, gray, green, orange, pink, purple, red, white, yellow",
    "enlarging, tapering",
    "bulbous, club, cup, equal, rhizomorphs, rooted, missing",
    "fibrous, scaly, silky, smooth",
    "fibrous, scaly, silky, smooth",
    "brown, buff, cinnamon, gray, orange, pink, red, white, yellow",
    "brown, buff, cinnamon, gray, orange, pink, red, white, yellow",
    "partial, universal",
    "brown, orange, white, yellow",
    "none, one, two",
    "cobwebby, evanescent, flaring, large, none, pendant, sheathing, zone",
    "black, brown, buff, chocolate, green, orange, purple, white, yellow",
    "abundant, clustered, numerous, scattered, several, solitary",
    "grasses, leaves, meadows, paths, urban, waste, woods"
  ),
  `Included levels` = c(
    "bell, conical, convex, flat, knobbed, sunken",
    "-",
    "brown (includes brown, buff, cinnamon), gray, green, pink (includes pink, purple, red), white, yellow",
    "yes, no",
    "-",
    "-",
    "-",
    "-",
    "black, brown (includes brown, buff, chocolate), gray, green, orange, pink (includes pink, purple, red), white, yellow",
    "enlarging, tapering",
    "-",
    "-",
    "-",
    "-",
    "-",
    "-",
    "-",
    "none, one, two",
    "-",
    "-",
    "group, solitary",
    "grasses (includes grasses, meadows), woods (includes woods, leaves), urban (includes paths, urban), waste"
  )
)

table1 %>%
  kbl(format = "html", caption = "Table 1. Predictors in the dataset.") %>% 
  kable_styling("striped") 
```

## Methods

We split the dataset into a training dataset, which will be used to fit the candidate models, and a testing dataset, which will be used to evaluate and compare the models. The training dataset contains 80% of the observations in the full dataset. All models will be evaluated using something something (metrics & justification)

```{r test and train} 
# set seed 
set.seed(191)

# test and train
n <- nrow(mushrooms)
mushrooms_split <- mushrooms %>% 
                   mutate(split = sample(rep(c("train", "test"), 
                                             times = c(round(.8*n), round(.2*n)))))

mushrooms_train <- mushrooms_split %>% 
                  filter(split == "train") %>%
                  dplyr::select(-split)

mushrooms_test  <- mushrooms_split %>% 
                  filter(split == "test") %>%
                  dplyr::select(-split)
```

```{r}
# beginner 
beginner       <- mushrooms %>% 
                  dplyr::select(class, cap.shape, cap.color, bruises, gill.color, 
                                stalk.shape, ring.number, population, habitat)

beginner_train <- mushrooms_train %>% 
                  dplyr::select(class, cap.shape, cap.color, bruises, gill.color, 
                                stalk.shape, ring.number, population, habitat)

beginner_test  <- mushrooms_test %>% 
                  dplyr::select(class, cap.shape, cap.color, bruises, gill.color,
                                stalk.shape, ring.number, population, habitat)
                

# advanced 
advanced       <- mushrooms 
advanced_train <- mushrooms_train
advanced_test  <- mushrooms_test
```

```{r}
rm(data, mushrooms, mushrooms_split, mushrooms_test, mushrooms_train)
```


### Logistic regression

As mentioned in the introduction it is the goal of this assignment to make a mushroom hunt available for everyone. This means that we need to test how many characteristics are needed to reach a satisfying level of accuracy. As we are beginners ourselve and we tried to identify some mushrooms based on the given characteristics we follow the credo of using the least amount of predictors as possible. 
For this reason we apply a dimension reduction technique which compares different combinations of predictors and compared on a chosen metric gives us the best set of predictors for our classification problem. 

As our main metric to choose the set of predictors we use the balanced accuracy $(Sensitifity + Specificity)/2$, but adjust it a little bit to also account for our goal to mainly classify mushrooms which are actually poisonous as poisonous. That means avoiding to have a lot of false negatives (mushrooms which are actually poisonous, but are classified as edible). For this sake we are even willing to miss classify more mushrooms which are actually edible as poisonous. Therefore, our choice of evaluation metric is a weighted balanced accuracy, where we ive more weight to the sensitivity as this should decrease the false negative rate $(Sensitifity*1.5 + Specificity)/2$.

For this selection of predictors we also wanted to test different thresholds for classifying a probability based on the given model as edible or poisonous. Therefore, we applied different thresholds and choose for each set of predictors the threshold were our chosen metric has its maximum value. 

In order to not rely on chance in our selection we applied 5-fold cross-validation and averaged the results over the folds. In this way we obtained the best subset of predictors as well as the corresponding threshold value based on the training set. 

Next we applied cross-validation again to compare the performance first on the training set between the different candidate models and also evaluated the performance on the test set. 

For these comparisons we mainly took the sensitivity, accuracy and our chosen metric into account.

We believe that we identified the best set of predictors based on the training data and that in this way we are also able to make a decision on how many predictors are actually necessary to get a satisfying performance. 

#### Generating all possible combination of predictors

```{r}
generate_formulas <- function(p, x_vars, y_var) {
  x_vars <- colnames(x_vars)
  # Input checking
  if (p %% 1 != 0)           stop("Input an integer n")
  if (p > length(x_vars))    stop("p should be smaller than number of vars")
  if (!is.character(x_vars)) stop("x_vars should be a character vector")
  if (!is.character(y_var))  stop("y_vars should be character type")
  
  # combn generates all combinations, apply turns them into formula strings
  apply(combn(x_vars, p), 2, function(vars) {
    paste0(y_var, " ~ ", paste(vars, collapse = " + "))
  })
}
```

#### Function to find best predictors and best alpha based on chosen metric (Balance from sens and spec with more weight for sens)

```{r}
# this formula should not be used for itself, but just in combination with the cross validation formula for finding the best predictors
find_best_predictors <- function(formulas,train,valid, valid_y){
  
  out <-data.frame(matrix(nrow = length(formulas), ncol = 3))
  thres <- data.frame(seq(0.05,0.95,length.out=19))
  alpha <- seq(0.05,0.95,length.out=19)

  for(i in 1:length(formulas)){
    model <- glm(formulas[i], family=binomial, data=train)
    pred_prob <- predict(model, type = "response", newdata = valid)
    
    comb <- data.frame(matrix(nrow = nrow(thres), ncol = 3))
    comb$alpha <- alpha
    colnames(comb) <- c("TPR", "TNR","mean","alpha")
      
    for(j in 1:nrow(thres)){
      pred_lr <- c()
      pred_lr <- case_when(pred_prob > as.numeric(thres[j,1]) ~ 1, pred_prob <= as.numeric(thres[j,1]) ~ 0)
      cmat_lr <- table(true = valid_y, predicted = pred_lr)
      if(sum(pred_lr == 1 ) == length(valid_y)){
        TN <- cmat_lr[1, 1]
        FN <- cmat_lr[2, 1]
        FP <- 0
        TP <- 0
      }
      else if(sum(pred_lr == 0 ) == length(valid_y)){
        TN <- 0
        FN <- 0
        FP <- cmat_lr[1, 1]
        TP <- cmat_lr[2, 1]
      }      else{
        TN <- cmat_lr[1, 1]
        FN <- cmat_lr[2, 1]
        FP <- cmat_lr[1, 2]
        TP <- cmat_lr[2, 2]
      }
      comb[j,1] <- TP / (TP + FN)
      comb[j,2] <- 1- (FP / (TN + FP)) # Same as TNR just to emphasize
      comb[j,3] <- (comb[j,1]+comb[j,2]*1.5)/2
      }
    comb <- comb[order(comb$mean, decreasing=T),]
    
    out[i,1] <- comb[1,3]
    out[i,2] <- formulas[i]
    out[i,3] <- comb[1,4]
    
    }
  colnames(out) <- c("meanTPR_TNR", "formula","alpha")
  
return(out)
}
```

#### Cross-validating the choice of predictors
```{r}
cv_best_pred <- function(k, dataset, form, top){
  
  Y <- data.frame(matrix(nrow = length(form), ncol = k))
  Z <- data.frame(matrix(nrow = length(form), ncol = k))
  Final <- data.frame(matrix(nrow = length(form), ncol = 3))
  
  # first, add a selection column to the dataset as before
  n_samples  <- nrow(dataset)
  select_vec <- rep(1:k, length.out = n_samples)
  data_split <- dataset %>% mutate(folds = sample(select_vec))
  
  for (i in 1:k) {
    # split the data in train and validation set
    data_train <- data_split %>% filter(folds != i)
    data_valid <- data_split %>% filter(folds == i)
    
    data_valid_y <- data_valid$class
    
    X <- find_best_predictors(formulas=form,train=data_train,valid=data_valid,valid_y=data_valid_y)
    Y[,i] <- X$meanTPR_TNR
    Z[,i] <- X$alpha
  }
  Final[,1]  <- X[[2]]
  Final[,2] <- rowMeans(Y)
  Final[,3] <- rowMeans(Z)
  Final <- Final[order(Final[2],decreasing = T),]
  
  colnames(Final) <- c("Formula", "CV_Mean of TNR_TPR"," CV_alpha")
  return(Final[1:top,])
}

```

#### Cross validating the chosen model and compare it to other models
```{r}
cv_best_models <- function(k, dataset, form, alpha, seed){
  
  set.seed(seed)
  
  comb <- data.frame(matrix(nrow = k, ncol = 3))
  colnames(comb) <- c("TPR", "TNR","Metric")
  Final <- data.frame(matrix(nrow = length(form), ncol = 3))
  
  # first, add a selection column to the dataset as before
  n_samples  <- nrow(dataset)
  select_vec <- rep(1:k, length.out = n_samples)
  data_split <- dataset %>% mutate(folds = sample(select_vec))
  
  for (i in 1:k) {
    # split the data in train and validation set
    data_train <- data_split %>% filter(folds != i)
    data_valid <- data_split %>% filter(folds == i)
    
    data_valid_y <- data_valid$class
    
    model <- glm(form, family=binomial, data= data_train)
    pred_prob <- predict(model, type = "response", newdata = data_valid)
    pred_lr <- ifelse(pred_prob > as.numeric(alpha), 1,0)
    cmat_lr <- table(true = data_valid_y, predicted = pred_lr)
    
    TN <- cmat_lr[1, 1]
    FN <- cmat_lr[2, 1]
    FP <- cmat_lr[1, 2]
    TP <- cmat_lr[2, 2]
    
    comb[i,1] <- TP / (TP + FN)
    comb[i,2] <- TN / (TN + FP)
    comb[i,3] <- (comb[i,2]*1.5+comb[i,1])/2
  }
  Final[,1]  <- mean(comb[,1])
  Final[,2] <- mean(comb[,2])
  Final[,3] <- mean(comb[,3])
  
  colnames(Final) <- c("CV_TPR", "CV_TNR","CV_Metric")
  
  return(Final)
}
```

#### Training Set results from the chosen models
```{r}
test_metric <- function(formula, train, test,alpha){
  fit1 <- glm(formula, family=binomial, data=train)
  pred_prob <- predict(fit1, type = "response", newdata = test)
  pred_lr   <- ifelse(pred_prob > as.numeric(alpha), 1,0)
  confm <- table(true = test$class, predicted = pred_lr)
  Spec <- confm[2, 2] / (confm[2, 2] + confm[2, 1])
  Sens <- confm[1, 1] / (confm[1, 1] + confm[1, 2])
  Metric <- (Sens*1.5+Spec)/2
  return(list(matrix=confm, Specificty=Spec, Sensitivity=Sens, Metric=Metric))
}
```

#### Applying the formulas
```{r}
mushrooms_begin_x <- beginner %>%
  dplyr::select(-class)
```

```{r}
# Generating the combination of formulas
formulas_2 <- generate_formulas(p=2,x_vars=mushrooms_begin_x, y_var="class")
formulas_3 <- generate_formulas(p=3,x_vars=mushrooms_begin_x, y_var="class")
formulas_4 <- generate_formulas(p=4,x_vars=mushrooms_begin_x, y_var="class")
formulas_5 <- generate_formulas(p=5,x_vars=mushrooms_begin_x, y_var="class")
formulas_6 <- generate_formulas(p=6,x_vars=mushrooms_begin_x, y_var="class")
```


```{r}
# generating the cross-validated results for the best choice of predictors
pred2 <- cv_best_pred(5, beginner_train, formulas_2, 5)
pred3 <- cv_best_pred(5, beginner_train, formulas_3, 5)
pred4 <- cv_best_pred(5, beginner_train, formulas_4, 5)
pred5 <- cv_best_pred(5, beginner_train, formulas_5, 5)
pred6 <- cv_best_pred(5, beginner_train, formulas_6, 5)
```

```{r}
# comparing the results 
pred2
pred3
pred4
pred5
pred6
# not a huge improvement between 3, 4, and 5
# All of these models would yield good results
```

```{r}
# comparison between models
cv_best_models(5, beginner_train, pred4[1,1], pred4[1,3], 123)
cv_best_models(5, beginner_train, pred5[1,1], pred5[1,3], 123) # this seems to be the best model
cv_best_models(5, beginner_train, pred6[1,1], pred6[1,3], 123)
```


```{r}
# assessing test set predictions
test_metric(pred5[1,1], beginner_train, beginner_test,pred5[1,3]) 
```

```{r}
# Final formula for the trees
pred5[1,1]
```

### Trees

something something, what are we doing and why and how are we making it more complex

```{r}
```

## Results

something something about the performance of each model

compare the logistic regressions with each other
compare the trees with each other

do we need to do any advanced model (no these are good)

## Discussion

pros and cons of regression and trees, which one we want to use
